{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b239509",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0815cf99",
   "metadata": {},
   "source": [
    "## What is a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec961511",
   "metadata": {},
   "source": [
    "- A “model” is a general specification of relationships among variables. \n",
    "    + e.g. a linear regression, such as: $ Price = \\beta_1*Time +  \\beta_0 (+ \\epsilon)$\n",
    "- A “trained model” is a particular model that has been built using some training data.\n",
    "    + If the model is **parametric** (like a linear regression), then it has parameters that have been calculated using the training data;\n",
    "    + If the model is **non-parametric**, then it has (not parameters but) an algorithm that has been constructed using the training data.\n",
    "    \n",
    "![Img](https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2019/04/Parametric-vs-Non-Parametric-model-Artificial-Intelligence-Interview-Questions-Edureka.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf214c1",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec51a8db",
   "metadata": {},
   "source": [
    "For two variables $X$ and $Y$, each with $n$ values:\n",
    "\n",
    "$\\Large\\sigma_{XY} = \\frac{\\Sigma^n_{i = 1}(x_i - \\mu_x)(y_i - \\mu_y)}{n}$ <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2bb429",
   "metadata": {},
   "source": [
    "`np.cov(X, Y, ddof=0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a07906a",
   "metadata": {},
   "source": [
    "Pearson Correlation:<br/>$\\Large r_P = \\frac{\\Sigma^n_{i = 1}(x_i - \\mu_x)(y_i - \\mu_y)}{\\sqrt{\\Sigma^n_{i = 1}(x_i - \\mu_x)^2\\Sigma^n_{i = 1}(y_i -\\mu_y)^2}}$\n",
    "\n",
    "Note that we are simply standardizing the covariance by the standard deviations of X and Y (the $n$'s cancel!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33196327",
   "metadata": {},
   "source": [
    "`np.corrcoef(X, Y)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d833f4ec",
   "metadata": {},
   "source": [
    "**Where X and Y are lists of X and Y values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a2b608",
   "metadata": {},
   "source": [
    "Similarly, you can use SciPy:<br>\n",
    "`stats.pearsonr(X, Y)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6763ab",
   "metadata": {},
   "source": [
    "### Regression equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48cf703",
   "metadata": {},
   "source": [
    "The solution for a simple regression best-fit line is as follows:\n",
    "\n",
    "- slope: <br/>$\\Large m = r_P\\frac{\\sigma_y}{\\sigma_x} = \\frac{cov(X, Y)}{var(X)}$\n",
    "\n",
    "- y-intercept:<br/> $\\Large b = \\mu_y - m\\mu_x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb49120",
   "metadata": {},
   "source": [
    "### Using Stats Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42fec16",
   "metadata": {},
   "source": [
    "`sm.formula.ols(formula=\"y ~ x\", data=test_df).fit().summary()` <br>\n",
    "Where your x:y data is in a dataframe.<br><br>\n",
    "or with seperate x and y, you can use:<br>\n",
    "`sm.OLS(y,x)` where y is a dataframe with y values and x is a dataframe with x values.\n",
    "\n",
    "Use `sm.add_constant(df[x])` to add a constant column into a new dataframe for x when using `sm.OLS`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d14db9",
   "metadata": {},
   "source": [
    "### $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a1c3c",
   "metadata": {},
   "source": [
    "$R^2$, the *coefficient of determination*, is a measure of how well the model fits the data.<br>\n",
    "How well variation in x explains variation in y.\n",
    "\n",
    "Interpreted as a percentage of change in y.\n",
    "\n",
    "\"$R^2$ change in Y is explained by change in X\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d2a4f3",
   "metadata": {},
   "source": [
    "The actual calculation of $R^2$ is: <br/> $\\Large R^2\\equiv 1-\\frac{\\Sigma_i(y_i - \\hat{y}_i)^2}{\\Sigma_i(y_i - \\bar{y})^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de813c16",
   "metadata": {},
   "source": [
    "Adjusted $R^2$ adds a penalty for the complexity of the model. <br>\n",
    "Adding more predictors increases the penalty dependent on the significance of the added predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7a163b",
   "metadata": {},
   "source": [
    "### Using `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d981c",
   "metadata": {},
   "source": [
    "`from sklearn.linear_model import LinearRegression`\n",
    "\n",
    "Initialize the regression by setting a variable = `LinearRegression()`\n",
    "\n",
    "Then fit the model using:<br>\n",
    "`variable.fit(df['x'], y)`\n",
    "\n",
    "if x is one dimensional, you made need to use `lr.fit(X.reshape(-1, 1), y)`\n",
    "\n",
    "For your $\\beta_1$ or x coefficient, `variable.coef_`\n",
    "\n",
    "For your $\\beta_o$ or y-intercept, `variable.intercept_`\n",
    "\n",
    "To access the model `variable.predict(X.reshape(-1, 1))`\n",
    "\n",
    "**$R^2$**\n",
    "\n",
    "To get your $R^2$ using `sklearn`, use<br>\n",
    "```\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y, model)\n",
    "```\n",
    "\n",
    "where model is your model from the previous section\n",
    "\n",
    "**Other metrics**\n",
    "\n",
    "To get other metrics about your model, use <br>\n",
    "```\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "print(\"Metrics:\")\n",
    "# R2\n",
    "print(f\"R2: {r2_score(y, model):.3f}\")\n",
    "# MAE\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y, model):.3f}\")\n",
    "# MSE\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y, model):.3f}\")\n",
    "# RMSE - just MSE but set squared=False\n",
    "print(f\"Root Mean Squared Error: {mean_squared_error(y, model, squared=False):.3f}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c4941",
   "metadata": {},
   "source": [
    "### Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f36a8de",
   "metadata": {},
   "source": [
    "#### Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7b0ea",
   "metadata": {},
   "source": [
    "**The relationship between the target and predictor is linear.** Check this by drawing a scatter plot of your predictor and your target, and see if there is evidence that the relationship might not follow a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ca329",
   "metadata": {},
   "source": [
    "To test, run Pearson's r Coefficient and scatterplot\n",
    "\n",
    "To test multicollinearity:\n",
    "\n",
    ".corr(), heatmap or **Variance Inflation Factor (VIF)** using:<br>\n",
    "```\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# defining an empty dataframe to capture the VIF scores\n",
    "vif = pd.DataFrame()\n",
    "\n",
    "# For each column,run a variance_inflaction_factor against all other columns to get a VIF Factor score\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X_train.values, i) for i in range(len(X_train.columns))]\n",
    "\n",
    "# label the scores with their related columns\n",
    "vif[\"features\"] = X_train.columns\n",
    "```\n",
    "\n",
    "$VIF < 10$ does not have high multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138e7d74",
   "metadata": {},
   "source": [
    "#### Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce34a3",
   "metadata": {},
   "source": [
    "**The errors are independent**. In other words: Knowing the error for one point doesn't tell you anything about the error for another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b33273",
   "metadata": {},
   "source": [
    "#### Normality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947f648d",
   "metadata": {},
   "source": [
    "**The errors are normally distributed.** That is, smaller errors are more probable than larger errors, according to the familiar bell curve.\n",
    "\n",
    "Use QQ plot with residuals. Residuals are calculated as:\n",
    "$$ residual = y-actual - y-predicted $$\n",
    "\n",
    "```\n",
    "# QQ plots are generally great tools for checking for normality.\n",
    "import statsmodels.api as sm\n",
    "\n",
    "sm.qqplot(train_residuals, line = 'r');\n",
    "```\n",
    "We want the points to fit the plotted line.\n",
    "\n",
    "A histogram of residuals should also be normally distributed around 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b19175",
   "metadata": {},
   "source": [
    "#### Homoskedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f02464",
   "metadata": {},
   "source": [
    "**The errors are homoskedastic.** That is, the errors have the same variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bf6d5e",
   "metadata": {},
   "source": [
    "To test for homoskedasticity, scatter plot residuals.\n",
    "\n",
    "Residuals should be pattern-less.\n",
    "\n",
    "- **The ideal scenario**\n",
    "\n",
    "    - Random scatter\n",
    "    - Scattered around 0\n",
    "    - No identifiable trend\n",
    "    \n",
    "    <img src=\"images/normal-resid.png\" width=\"550\">  \n",
    "    \n",
    "- **Non-linear relationship**\n",
    "\n",
    "    - Clear non-linear scatter, but\n",
    "    - Identifiable trend\n",
    "    - **Fix:** Introduce polynomial terms\n",
    "    - **Fix:** Variable transformation\n",
    "    \n",
    "    <img src=\"images/polynomial-resid.png\" width=\"550\">\n",
    "\n",
    "- **Autocorrelation**\n",
    "\n",
    "    - Identifiable trend, or\n",
    "    - Consecutively positive/negative residuals\n",
    "    - **Fix:** Consider sequential analysis methods (which we'll discuss in phase 4)\n",
    "    \n",
    "    <img src=\"images/autocorrelation.png\" width=\"550\">\n",
    "\n",
    "- **Heteroskedasticity**\n",
    "\n",
    "    - The spread of residuals is different at different levels of the fitted values\n",
    "    - **Fix:** Variable transformation (log)  \n",
    "    \n",
    "    <img src=\"images/heteroskedasticity.png\" width=\"550\">\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67605ef9",
   "metadata": {},
   "source": [
    "For interpretation after **log transformation**, interpret coef as:\n",
    "\n",
    "Change of 1 in $x$ is a $(e^{coef} -1) \\cdot 100$ % change in $y$ **holding all else constant**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5294d87f",
   "metadata": {},
   "source": [
    "## Multiple Linear Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66af4d5",
   "metadata": {},
   "source": [
    "Can add categorical vars to regression.\n",
    "\n",
    "We are also adding predictive variables, so our regression equation, then, instead of looking like $\\hat{y} = mx + b$, will now look like:\n",
    "\n",
    "$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + ... + \\hat{\\beta}_nx_n$.\n",
    "\n",
    "Remember that the hats ( $\\hat{}$ ) indicate parameters that are estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322996fd",
   "metadata": {},
   "source": [
    "**Closed-form solution (matrices)**<br>\n",
    "In a word, for a multiple linear regression problem where $X$ is the matrix of independent variable values and $y$ is the vector of dependent variable values, the vector of optimizing regression coefficients $\\vec{b}$ is given by:\n",
    "\n",
    "$\\vec{b} = (X^TX)^{-1}X^Ty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd40a225",
   "metadata": {},
   "source": [
    "**Dummying categorical data**\n",
    "\n",
    "Avoid dummying numeric variables or variables of high cardinality.\n",
    "\n",
    "You can bin variables of high cardinality to reduce the number of coefficients created by dummying that variable.\n",
    "\n",
    "Should result in a table of 0s and 1s for categorical data.\n",
    "\n",
    "Use `pd.get_dummies()` to create dummy columns for a given df\n",
    "\n",
    "or use One Hot as per example:\n",
    "```\n",
    "# Let's try using sklearn's OneHotEncoder to create our dummy columns:\n",
    "\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "comma_trans = ohe.fit_transform(comma_use.drop('RespondentID', axis=1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba47916",
   "metadata": {},
   "source": [
    "### Finding your correlated $x$'s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b551487",
   "metadata": {},
   "source": [
    "When choosing features, you can use a correlation table:<br>\n",
    "`df.corr()`\n",
    "\n",
    "and plot a heat map using seaborn<br>\n",
    "```\n",
    "sns.set(rc={'figure.figsize':(8, 8)})\n",
    "\n",
    "# Use the .heatmap method to depict the relationships visually!\n",
    "sns.heatmap(wine.corr());\n",
    "```\n",
    "\n",
    "but the best way to find the predictors with the highest correlations you can use this:\n",
    "```\n",
    "df_corrs = df.corr()['y'].map(abs).sort_values(ascending=False)\n",
    "df_corrs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d477ec69",
   "metadata": {},
   "source": [
    "### Recursive feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09e42f",
   "metadata": {},
   "source": [
    "Removes lowest correlating predictors in data set up to a given number of features remaining.\n",
    "\n",
    "```\n",
    "select = RFE(lr_rfe, n_features_to_select=3)\n",
    "\n",
    "# Fit standard scaled model on X and y-variable\n",
    "select.fit(X=df_scaled, y=df['y'])\n",
    "\n",
    "# Run ranking on your columns.\n",
    "select.ranking_\n",
    "\n",
    "```\n",
    "\n",
    "> **Caution**: RFE is probably not a good strategy if your initial dataset has many predictors. It will likely be easier to start with a *simple* model and then slowly increase its complexity. This is also good advice for when you're first getting your feet wet with `sklearn`!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf2e587",
   "metadata": {},
   "source": [
    "### Standard Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64dbb9a",
   "metadata": {},
   "source": [
    "Benefits:\n",
    "\n",
    "- This tends to make values relatively small (mean value is at $0$ and one standard deviation $\\sigma$ from the mean is $1$).\n",
    "- **Easier interpretation: larger coefficients tend to be more influential**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ed896",
   "metadata": {},
   "source": [
    "Use the following method to standardize the scale of your data:\n",
    "```\n",
    "df_scaled = (df - np.mean(df)) / np.std(df)\n",
    "```\n",
    "Essentially, you're taking a z-score for every value to standardize the coefs.\n",
    "\n",
    "Thus, the coefs can be interpreted as for every $\\sigma$ increase in $x$, $y$ changes by coef."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857fcd7",
   "metadata": {},
   "source": [
    "With coefs standardized, you can evaluate and remove predictors of low magnitude contributing predictors to reduce model complexity.\n",
    "\n",
    "To standard scale in `sklearn` use \n",
    "```\n",
    "ss = StandardScaler()\n",
    "ss.fit(df)\n",
    "data_std_scale = ss.transform(df)\n",
    "```\n",
    "Once the data is standardized, you can fit it with a linear regression:\n",
    "```\n",
    "lr = LinearRegression()\n",
    "lr.fit(data_std_scale, test_df)\n",
    "```\n",
    "\n",
    "And get your metrics using the following code:\n",
    "```\n",
    "print(\"Metrics:\")\n",
    "# Intercept\n",
    "print(f\"y-intercept: {lr.intercept_:.3f}\")\n",
    "# Coefs\n",
    "print(f\"x coefficients: {lr.coef_:.3f}\")\n",
    "# R-Squared\n",
    "print(f\"R2: {lr.score(data_std_scale, test_df):.3f}\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1534cea",
   "metadata": {},
   "source": [
    "## Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce27aca",
   "metadata": {},
   "source": [
    "Predictive models are more concerned with the output of a model ($y$) rather than coefs or intercepts.\n",
    "\n",
    "Because of this, we're not concerned with transformations or number of predictors. More predictors is probably better.\n",
    "\n",
    "We can also forego some of our linear regression assumptions that we must hold for inferential statistics.\n",
    "\n",
    "Primary concern is reducing error.\n",
    "\n",
    "Thus, we want to use the following to evaluate our model:\n",
    "- Root Mean Error\n",
    "- Mean Absolute Error\n",
    "- Mean Squared Error\n",
    "\n",
    "$\\Large Total\\ Error\\ = Prediction\\ Error+ Irreducible\\ Error$\n",
    "\n",
    "Our prediction error can be further broken down into error due to bias and error due to variance.\n",
    "\n",
    "$\\Large Total\\ Error = Model\\ Bias^2 + Model\\ Variance + Irreducible\\ Error$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f0d448",
   "metadata": {},
   "source": [
    "### Variance vs Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51cc528",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
